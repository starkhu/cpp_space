# 模型训练的历史回顾

模型训练伴随着深度学习领域的发展而成长，二者互相依赖，互相成就。只有训练出更大更好的模型才能获取更好的应用效果。这里回顾一下模型训练的成长历史，让我们从一个和尚担水开始。

```
话说，从前山上有座庙，庙很小，一个和尚下山担水就能满足寺里的用水需求；
随着寺庙声名在外，香火逐渐兴旺，需要多个和尚下山担水才能满足寺里的用水需求；
山路崎岖，走的时间长了，小和尚们发现了一条羊肠小道， 也可直通山下溪流，小道崎岖，担水会有些许洒落，但小和尚们也能欣然接受；
后来寺庙香火更加鼎盛，小和尚们发现依靠水桶担水已经无法满足寺里的用水需求了，于是他们造了一辆水车，水车需要多个小和尚协调操作，但是运水效率比小和尚们担水要高多了。
```

## 模型壮大的成长史
**小庙，一个和尚担水**

深度学习模型发展伊始，是2012年左右，卷积神经网络Alexnet在ImageNet竞赛上拔得头筹，top-5的error rate是15.3， 而第二名是26.2%。那时候模型都很小， 一张卡就能轻松训练起来。

**香火兴旺，多个和尚担水**

随着Alexnet网络的走红，越来越多的模型卷积神经网络涌现出来，模型的层数开始越堆越高， 参数规模越来越大，单次迭代的时间也越来越长，如果仍然用单卡训练模型会导致模型训练时间过长，而无法接受。这时候需要多张卡并行训练来，用来加速模型的训练。

**羊肠近道，事半功倍**

随着模型的越来越大，工程师尝试使用半精度进行训练，发现大部分使用半精度训练后的网络的准确度和浮点型训练的准确度差不多。即使精度略有下降，差距也很小。

**香火鼎盛，水车革命**

随着模型规模更加庞大，发现单卡的内存已经不能满足模型需要消耗的内存了，这时候工程师们不得不将模型协调分散到多个模型中进行训练。工程师提出了ZeRO数据并行/模型并行/流水并行等方案来解决大规模的训练。

## 模型的解决方案路线
深度学习领域的快速发展离不开深度学习框架的功劳，这是深度学习框架提供了简易的python接口，完善的算子库，帮助工程师们轻松上手模型训练。以下罗列了面对不同的训练需求，框架们呈现的解决方案。

|序号|模式| 解决方案路线1 | 解决方案路线2 | 担水史 |
|---|---|---|---|---|
| 1 |单机单卡| tensorflow | pytorch | 一个和尚担水 |
| 2 |单机多卡| Horovod | DDP/Horovod | 多个和尚担水 |
| 3 |混合精度训练| amp | apex | 羊肠小道 |
| 4 |大规模分布式训练| - | DeepSpeed/Megatron | 水车革命 |


